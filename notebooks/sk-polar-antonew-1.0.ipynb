{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Process Glove. Ignore if mod file exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.42B.300d.zip\n",
    "!unzip glove*.zip\n",
    "\n",
    "model_glove = glove2word2vec('glove.42B.300d.txt','gensim_glove_300d.txt')\n",
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_300d.txt\", binary=False)\n",
    "\n",
    "def generate_norm_embedding(model, output_path):\n",
    "    temp_file = open(output_path,'wb')\n",
    "    temp_file.write(str.encode(str(len(model.vocab))+' '+str(model.vector_size)+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        temp_file.write(model[each_word]/linalg.norm(model[each_word]))\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "    \n",
    "    temp_file.close()\n",
    "\n",
    "generate_norm_embedding(model_glove,'glove_norm_300.mod')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load processed glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('/Users/stjepankusenic/POLAR_WEBE/notebooks/glove_norm_300.mod',binary=True) # ss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_model = model_gn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and process antonyms. Ignore if final_antonym_list exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "with open('Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load final_antonym_list as antonym list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_antonym = pd.read_pickle(r'final_antonym_list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_new= [('product', 'service'),\n",
    "('essential', 'luxury'),\n",
    "('technical', 'natural'),\n",
    "('renewable', 'nonrenewable'),\n",
    "('advertising', 'rumour'), \n",
    "('lease', 'sell'), \n",
    "('tangible', 'intangible'), \n",
    "('demand', 'supply'),\n",
    "('wfh', 'wfo'),\n",
    "('child', 'childless'),\n",
    "('remote', 'physical'),\n",
    "('salary', 'goodies'),\n",
    "('store', 'online'),\n",
    "('details', 'outlines'),\n",
    "('stakeholders', 'spectators'),\n",
    "('isolating', 'social'),\n",
    "('goal', 'task'),\n",
    "('employees', 'consultant'),\n",
    "('cost', 'revenue'),\n",
    "('seasonal', 'temporary'),\n",
    "('alliance', 'proprietorship'),\n",
    "('loss', 'profit'),\n",
    "('integrity', 'corruption'),\n",
    "('international', 'local'),\n",
    "('corporate', 'individual'),\n",
    "('order', 'disorder'),\n",
    "('solution', 'problem'),\n",
    "('manager', 'worker'),\n",
    "('diversity', 'uniformity'),\n",
    "('public', 'private'),\n",
    "('strategic', 'impulsive'),\n",
    "('innovator', 'follower'),\n",
    "('bankruptcy', 'prosperity'),\n",
    "('growth', 'decline'),\n",
    "('sustainable', 'unsustainable'),\n",
    "('family', 'work'),\n",
    "('criminal', 'rightful'),\n",
    "('financial', 'artisanal'),\n",
    "('supplier', 'purchaser'),\n",
    "('commitment', 'rejection'),\n",
    "('professional', 'amateur'),\n",
    "('independent', 'dependent'),\n",
    "('digital', 'analogue'),\n",
    "('marketing', 'secret'),\n",
    "('secure', 'risky'),\n",
    "('longterm', 'shortterm'), \n",
    "('responsible', 'neglect'), \n",
    "('ethical', 'unethical'), \n",
    "('beneficial', 'harmful'), \n",
    "('diversity', 'uniformity'), \n",
    "('trust', 'mistrust'), \n",
    "('teamwork', 'individualism'), \n",
    "('opportunity', 'threat'), \n",
    "('innovative', 'traditional'), \n",
    "('flexible', 'rigid'), \n",
    "('ambiguity', 'clarity'), \n",
    "('feminine', 'masculine'), \n",
    "('globally', 'locally'), \n",
    "('insiders', 'outsiders'), \n",
    "('foreigners', 'natives'), \n",
    "('minorities', 'majority'),\n",
    "('transparency', 'obscurity'),\n",
    "('discrimination', 'impartial'),\n",
    "('credible', 'deceptive'),\n",
    "('environment', 'pollution'),\n",
    "('pressure', 'relax'),\n",
    "('growth', 'decline'),\n",
    "('satisfied', 'unsatisfied'),\n",
    "('diplomatic', 'undiplomatic'),\n",
    "('motivate', 'demotivate'),\n",
    "('communicative', 'uncommunicative'),\n",
    "('connected', 'disconnected'),\n",
    "('autonomous', 'micromanagement'),\n",
    "('nurture', 'neglect'),\n",
    "('progressive', 'conservative'),\n",
    "('rewarding', 'unrewarding'),\n",
    "('bias', 'unbias'),\n",
    "('challenge', 'obscurity'),\n",
    "('collaborated', 'siloed'),\n",
    "('outdated', 'modern'),\n",
    "('effortless', 'demanding'),\n",
    "('economic', 'overpriced'),\n",
    "('widespread', 'local'),\n",
    "('freedom', 'captive'),\n",
    "('consistent', 'inconsistent')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/2013486443.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_pair in tqdm(list_new):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637204420dfb48cd84201eac97535fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/2013486443.py:13: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_key in tqdm(similarity_matrix):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e468204b0b46298c4a05c662a76575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/2013486443.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index_counter, each_key in enumerate(tqdm(all_similarity)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7550de06da0840efbb681a8efc1b6fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "list_new= list(dict.fromkeys(list_new).keys())\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_new):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_list))\n",
    "\n",
    "list_antonym = final_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide on the size of the antonym vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82, 300)\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 82\n",
    "\n",
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    antonymy_vector.append(current_model[each_word_pair[0]]- current_model[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset Dimension Selection Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import scipy\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = scipy.spatial.distance.cdist(np.array(antonymy_vector),np.array(antonymy_vector),'cosine')\n",
    "dimension_similarity_matrix = abs(1-dimension_similarity_matrix)\n",
    "        \n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "        \n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "    \n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "    \n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "    \n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select POLAR dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the RANDOM DIMENSION Order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "random_antonym_vector = [i for i in range(len(antonymy_vector))]\n",
    "random.shuffle(random_antonym_vector)\n",
    "print(len(random_antonym_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the ORTHOGONAL DIMENSION Order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working list is ready, shape (82, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/1541790553.py:30: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for test_count in tqdm(range(num_dim-1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e9fb5e935e4782a5b5125b73631be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82,)\n"
     ]
    }
   ],
   "source": [
    "orthogonal_antonymy_vector =np.array(select_subset_dimension(antonymy_vector, num_antonym))  \n",
    "print(orthogonal_antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the MAXIMUM VARIANCE DIMENSION Order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding size is 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/2849266527.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "embedding_size = antonymy_vector.shape[0]   \n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "variance_antonymy_vector_inverse = torch.tensor(variance_antonymy_vector_inverse)\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "var_list = [None for x in range(20)] # variance for each antonym in each batch\n",
    "\n",
    "for i in range(19):  # the first 19 batches, each of size 100k\n",
    "  temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,100000*i:100000*i+100000])\n",
    "  temp_var_mean = torch.var(temp, axis = 1)\n",
    "  var_list[i] = temp_var_mean.numpy()\n",
    "  del temp\n",
    "\n",
    "temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,1900000:])\n",
    "temp_var_mean = torch.var(temp, axis = 1)\n",
    "var_list[19] = temp_var_mean.numpy()\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# using lazy approach. assume each batch is independent and the overall variance is the average variance over all batches\n",
    "\n",
    "variance_list = np.mean(np.array(var_list),axis = 0)\n",
    "\n",
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin POLAR embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "company = pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/interim/glove-ticker-fortune1000-us.csv')\n",
    "ticker_list = company['Ticker'].str.lower() # a list of tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ticker_word_embedding = dict()\n",
    "for ticker in ticker_list:    # dictionary of ticker: glove embedding\n",
    "  ticker_word_embedding[ticker] = current_model[ticker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "\n",
    "    temp_dict = dict()\n",
    "\n",
    "    embedding_size = current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "    \n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "        temp_dict[each_word] = new_vector\n",
    "        \n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    new_embedding_dict = transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)\n",
    "\n",
    "    return new_embedding_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of POLAR demensions and the method of antonym selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "dim_size = 82 # Number of POLAR dimenions\n",
    "antonym_vector_method = orthogonal_antonymy_vector \n",
    "# random_antonym_vector or orthogonal_antonymy_vector or variance_antonymy_vector \n",
    "antonym_500 = [list_antonym[x] for x in antonym_vector_method[:82]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model size is 892 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_59796/2921753199.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_word in tqdm(current_model):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36845ea4cc84dfba013338578fab33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/892 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create POLAR embedding of tickers\n",
    "ticker_new_embedding = generate_embedding_path(ticker_word_embedding, 'test_run',True ,antonym_vector_method,82) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View top embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_polar_dict(company_name, antonym, embedding, top_n = False, n = 10):\n",
    "  temp_dict = dict()\n",
    "  temp_polar = embedding[company_name]\n",
    "\n",
    "  if top_n:\n",
    "    idx = np.argsort([abs(x) for x in temp_polar])[-n:]\n",
    "    for i in idx:\n",
    "      print(antonym[i],temp_polar[i],'\\n')\n",
    "\n",
    "\n",
    "  if len(antonym) == len(temp_polar):\n",
    "    for a in range(len(antonym)):\n",
    "      temp_dict[antonym[a]] = temp_polar[a]\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spectators', 'stakeholders') -0.16217703 \n",
      "\n",
      "('foreigners', 'natives') -0.16648787 \n",
      "\n",
      "('analogue', 'digital') -0.1780018 \n",
      "\n",
      "('loss', 'profit') -0.17841296 \n",
      "\n",
      "('conservative', 'progressive') 0.19495878 \n",
      "\n",
      "('demand', 'supply') 0.19550358 \n",
      "\n",
      "('insiders', 'outsiders') 0.19992629 \n",
      "\n",
      "('artisanal', 'financial') -0.24137904 \n",
      "\n",
      "('economic', 'overpriced') -0.25286204 \n",
      "\n",
      "('beneficial', 'harmful') 0.28406265 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# amazon \n",
    "amzn_polar = make_polar_dict('amzn', antonym_500, ticker_new_embedding, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('modern', 'outdated') -0.19774586 \n",
      "\n",
      "('feminine', 'masculine') 0.19941388 \n",
      "\n",
      "('diplomatic', 'undiplomatic') -0.20358516 \n",
      "\n",
      "('nonrenewable', 'renewable') 0.21047162 \n",
      "\n",
      "('online', 'store') -0.21100266 \n",
      "\n",
      "('consistent', 'inconsistent') 0.22974993 \n",
      "\n",
      "('globally', 'locally') 0.24147882 \n",
      "\n",
      "('beneficial', 'harmful') 0.25350198 \n",
      "\n",
      "('consultant', 'employees') -0.27024293 \n",
      "\n",
      "('decline', 'growth') -0.27411142 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# walmart\n",
    "wmt_polar = make_polar_dict('wmt', antonym_500, ticker_new_embedding, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('manager', 'worker') 0.18059273 \n",
      "\n",
      "('problem', 'solution') 0.18642288 \n",
      "\n",
      "('intangible', 'tangible') -0.1915821 \n",
      "\n",
      "('details', 'outlines') 0.19724244 \n",
      "\n",
      "('analogue', 'digital') -0.21507092 \n",
      "\n",
      "('economic', 'overpriced') -0.25961307 \n",
      "\n",
      "('isolating', 'social') -0.26350588 \n",
      "\n",
      "('seasonal', 'temporary') -0.26964805 \n",
      "\n",
      "('consistent', 'inconsistent') 0.295854 \n",
      "\n",
      "('follower', 'innovator') 0.32891008 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# facebook\n",
    "fb_polar = make_polar_dict('fb', antonym_500, ticker_new_embedding, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('wfh', 'wfo') 0.18201385 \n",
      "\n",
      "('connected', 'disconnected') -0.18227455 \n",
      "\n",
      "('bankruptcy', 'prosperity') 0.18380293 \n",
      "\n",
      "('private', 'public') -0.1954872 \n",
      "\n",
      "('globally', 'locally') 0.20538466 \n",
      "\n",
      "('beneficial', 'harmful') 0.20755509 \n",
      "\n",
      "('ambiguity', 'clarity') -0.22157072 \n",
      "\n",
      "('decline', 'growth') -0.22775714 \n",
      "\n",
      "('consistent', 'inconsistent') 0.32833746 \n",
      "\n",
      "('demand', 'supply') -0.37309232 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# wells fargo\n",
    "wfc_polar = make_polar_dict('wfc', antonym_500, ticker_new_embedding, True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}