{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef5828a",
   "metadata": {},
   "source": [
    "# Generate Polar Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b57359",
   "metadata": {},
   "source": [
    "Change the importing of models and lists acording to which model you want to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de910e",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d0253f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import torch\n",
    "import subprocess\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec4194",
   "metadata": {},
   "source": [
    "### Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d99d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_glove = glove2word2vec('/Users/stjepankusenic/POLAR_WEBE/data/raw/glove.twitter.27B.200d.txt','gensim_glove_twitter_200d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format(\"/Users/stjepankusenic/POLAR_WEBE/data/raw/reddit_word2vec.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_norm_embedding(model, output_path):\n",
    "    temp_file = open(output_path,'wb')\n",
    "    temp_file.write(str.encode(str(len(model.vocab))+' '+str(model.vector_size)+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        temp_file.write(model[each_word]/linalg.norm(model[each_word]))\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "    \n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_norm_embedding(model_glove,'reddit_word2vec.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d940e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('/Users/stjepankusenic/POLAR_WEBE/data/raw/reddit_word2vec.mod',binary=True)\n",
    "current_model = model_gn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f1483c",
   "metadata": {},
   "source": [
    "### Import Polar Dimension List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2995a88",
   "metadata": {},
   "source": [
    "#####  Antonym Dimesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7fc914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load original antonyms\n",
    "list_antonym = pd.read_pickle(r'/Users/stjepankusenic/POLAR_WEBE/data/interim/final_antonym_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab2fc1",
   "metadata": {},
   "source": [
    "#### Business Dimesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579393ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load business antonym list\n",
    "list_new= [('product', 'service'), ('essential', 'luxury'), ('technical', 'natural'), #('renewable', 'nonrenewable'),\n",
    "           ('advertising', 'secretive'), ('lease', 'sell'), ('tangible', 'intangible'), ('demand', 'supply'), #('wfh', 'wfo'),\n",
    "           ('child', 'childless'), ('remote', 'physical'), ('salary', 'goodies'), ('store', 'online'), \n",
    "           ('details', 'outlines'), ('stakeholders', 'spectators'), ('isolating', 'social'), ('goal', 'task'),\n",
    "           ('employees', 'consultant'), ('cost', 'revenue'), ('seasonal', 'temporary'), ('alliance', 'proprietorship'),\n",
    "           ('loss', 'profit'), ('integrity', 'corruption'), ('international', 'local'), ('corporate', 'individual'),\n",
    "           ('order', 'disorder'), ('solution', 'problem'), ('manager', 'worker'), ('diversity', 'uniformity'),\n",
    "           ('public', 'private'), ('strategic', 'impulsive'), ('innovator', 'follower'), ('bankruptcy', 'prosperity'),\n",
    "           ('growth', 'decline'), ('sustainable', 'unsustainable'), ('family', 'work'), ('criminal', 'rightful'),\n",
    "           ('financial', 'artisanal'), ('supplier', 'purchaser'), ('commitment', 'rejection'), ('professional', 'amateur'),\n",
    "           ('independent', 'dependent'), ('digital', 'analogue'), ('marketing', 'secret'), ('secure', 'risky'), #('longterm', 'shortterm'), \n",
    "           ('responsible', 'neglect'), ('ethical', 'unethical'), ('beneficial', 'harmful'),\n",
    "           ('diversity', 'uniformity'), ('trust', 'mistrust'), ('teamwork', 'individualism'), ('opportunity', 'threat'),\n",
    "           ('innovative', 'traditional'), ('flexible', 'rigid'), ('ambiguity', 'clarity'), ('feminine', 'masculine'),\n",
    "           ('globally', 'locally'), ('insiders', 'outsiders'), ('foreigners', 'natives'), ('minorities', 'majority'),\n",
    "           ('transparency', 'obscurity'), ('discrimination', 'impartial'), ('credible', 'deceptive'), ('environment', 'pollution'),\n",
    "           ('pressure', 'relax'), ('growth', 'decline'), ('satisfied', 'unsatisfied'), #('diplomatic', 'undiplomatic'), ('motivate', 'demotivate'), ('communicative', 'uncommunicative'), \n",
    "           ('connected', 'disconnected'), #('autonomous', 'micromanagement'), \n",
    "           ('nurture', 'neglect'), ('progressive', 'conservative'),#('rewarding', 'unrewarding'), ('bias', 'unbias'), \n",
    "           ('challenge', 'obscurity'), ('collaboration', 'silo'),\n",
    "           ('outdated', 'modern'), ('effortless', 'demanding'), ('economic', 'overpriced'), ('widespread', 'local'),\n",
    "           ('freedom', 'captive'), ('consistent', 'inconsistent')]\n",
    "\n",
    "list_new= list(dict.fromkeys(list_new).keys())\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_new):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "\n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "final_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_list))\n",
    "\n",
    "list_antonym = final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b90ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(list_antonym)\n",
    "#type(current_model[list_antonym[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3ec26a",
   "metadata": {},
   "source": [
    "#### Image Rating Dimesions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_image= [('affordable','expensive'),('quality','worthless'),('limited','common'),('usefull','useless'),('many','few'),('demand','excess'),('valuable','worthless'),('current','outdated'),('simple','complex'), #product metrics 1-9\n",
    "             ('cheap','pricy'),('high','low'),('fair','unfair'),('total','partial'),('reasonable','unreasonable'),('accurate','inaccurate'),('clear','unclear'), #price metrics 10-16\n",
    "             ('store','online'),('crowded','empty'),('clean','dirty'),('innovative','conventional'),('organized','chaotic'),('bright','dull'),('convenient','inconvenient'),('attractive','unattractive'), #vendor metrics 17-24\n",
    "             ('friendly','rude'),('helpful','unhelpful'),('good','bad'),('calm','hectic'),('direct','vague'),('calmness','pressure'),('fast','slow'),('responsive','unresponsive'), #service metrics 25-32\n",
    "             ('wealthy','poor'),('intelligent','stupid'),('young','old'),('experienced','beginner'),('worker','manager'),('renowned','unknown'),('successful','unsuccessful'),('reliable','unreliable'),('enough','insufficient'), #workforce metrics 33-41\n",
    "             ('frequent','infrequent'),('interesting','boring'),('flashy','simple'),('meaningfull','meaningless'),('happy','serious'),('effective','ineffective'),('informative','uninformative'),('true','false'), #marketing metrics 42-49\n",
    "             ('honest','corrupt'),('environment','pollution'),('sustainable','unsustainable'),('big','small'),('helpful','harmful'),('relevant','irrelevant') #corporate metrics 50-55\n",
    "            ]\n",
    "len(list_image)\n",
    "list_image= list(dict.fromkeys(list_image).keys())\n",
    "\n",
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_image):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "\n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "\n",
    "final_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_list))\n",
    "\n",
    "list_antonym = final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bceb262",
   "metadata": {},
   "source": [
    "### Decide Polar Dimesion Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96e30dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1414, 100)\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 500\n",
    "\n",
    "## Find the antonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    if each_word_pair[0] in current_model.vocab:\n",
    "        if each_word_pair[1] in current_model.vocab:\n",
    "            antonymy_vector.append(current_model[each_word_pair[0]]- current_model[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855fb66",
   "metadata": {},
   "source": [
    "### Implement Dimesion Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572f67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = scipy.spatial.distance.cdist(np.array(antonymy_vector),np.array(antonymy_vector),'cosine')\n",
    "dimension_similarity_matrix = abs(1-dimension_similarity_matrix)\n",
    "\n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "\n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "\n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "\n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "\n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e91f4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding size is 1414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_3343/1149897526.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))\n"
     ]
    }
   ],
   "source": [
    "embedding_size = antonymy_vector.shape[0]\n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "variance_antonymy_vector_inverse = torch.tensor(variance_antonymy_vector_inverse)\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "current_model_tensor = torch.t(torch.tensor(current_model.wv.vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987c5e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = [None for x in range(20)] # variance for each antonym in each batch\n",
    "\n",
    "for i in range(19):  # the first 19 batches, each of size 100k\n",
    "  temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,100000*i:100000*i+100000])\n",
    "  temp_var_mean = torch.var(temp, axis = 1)\n",
    "  var_list[i] = temp_var_mean.numpy()\n",
    "  del temp\n",
    "\n",
    "temp = torch.matmul(variance_antonymy_vector_inverse, current_model_tensor[:,1900000:])\n",
    "temp_var_mean = torch.var(temp, axis = 1)\n",
    "var_list[19] = temp_var_mean.numpy()\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0169b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using lazy approach. assume each batch is independent and the overall variance is the average variance over all batches\n",
    "\n",
    "variance_list = np.mean(np.array(var_list),axis = 0)\n",
    "\n",
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d4640",
   "metadata": {},
   "source": [
    "### Import business entity names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60602bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "company = pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/raw/International_Fortune_GloVe.csv')\n",
    "name_list = company['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee49307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['walmart','homedepot','amazon','apple','cvs','toyota','volkswagen','berkshire','mckesson','samsung',\n",
    "             'ping','royal','industrial','alphabet','hon','exxon','daimler','costco','cigna','cardinal','microsoft',\n",
    "             'walgreens','allianz','kroger','jpmorgan','huawei','verizon','axa','ford','honda','general','anthem',\n",
    "             'mitsubishi','deutsche','bmw','nippon','saic','fannie','alibaba','comcast','amer','shandong','chevron',\n",
    "             'dell','bank','target','marathon','citigroup','hyundai','gazprom','facebook','royal','sony','johnson',\n",
    "             'hitachi','carrefour','bnp','bosch','tesco','aeon','hsbc','wells','general','state','intel','humana',\n",
    "             'nippon','deutsche','nissan','munich','enel','banco','procter','sk','pepsico','tencent','albertsons',\n",
    "             'basf','fedex','metlife','bank','aviation','freddie','greenland','phillips','lockheed','walt','archer',\n",
    "             'roche','xiamen','pacific','siemens','engie','legal','panasonic','reliance','brookfield','aviva','lenovo',\n",
    "             'valero','toyota','zurich','xiamen','aegon','boeing','unilever','guangzhou','prudential','airbus','mitsubishi',\n",
    "             'petrobras','hp','raytheon','softbank','prudential','tokyo','seven','alimentation','lg','goldman','industrial','aluminum',\n",
    "             'sysco','jbs','morgan','state','ptt','hca','tokio','vodafone','christian','aia','vinci','kia','eni',\n",
    "             'novartis','renault','shaanxi','cisco','korea','bayer','power','charter','merck','elo','shaanxi','zhejiang',\n",
    "             'denso','deutsche','publix','allstate','zhejiang','pemex','accenture','edeka','liberty','groupe','lloyds',\n",
    "             'tyson','bhp','woolworths','progressive','petronas','nationwide','pfizer','shandong','caterpillar','george',\n",
    "             'vale','acs','maersk','mitsubishi','ubs','oracle','energy','daiwa','jiangsu','zhejiang','dow','meiji',\n",
    "             'nike','zf','quanta','northrop','volvo','metro','usaa','chubb','banco','xiaomi','deere','barclays','cathay',\n",
    "             'mitsubishi','abbott','ck','poste','sncf','tata','fujitsu','cedar','northwestern','dollar','louis',\n",
    "             'jardine','magna','honeywell','bank','phoenix','credit','sun','thermo','repsol','tjx','shandong','travelers',\n",
    "             'capital','new','ing','tesla','cma','bharat','sap','shenzhen','coop','hyundai','anglo','mitsubishi','siemens',\n",
    "             'shanxi','jfe','haier','takeda','abb','suzuki','canon','new','samsung','kansai','enbridge','medtronic','toshiba',\n",
    "             'philip','arrow','schneider','banco','phoenix','chs','beijing','nec','zhejiang','bridgestone','guangxi',\n",
    "             'crh','xinjiang','linde','enterprise','mazda','hewlett','subaru','guangzhou','lg','kraft','guangzhou','olam',\n",
    "             'yunnan','samsung','wh','dollar','amgen','compass','coles','ericsson','banco','performance','netflix',\n",
    "             'nokia','bae','gree','gilead','eli','commonwealth','flex','rite']\n",
    "name_list = set(name_list)\n",
    "name_list = list(name_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1603f282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_word_embedding = dict()\n",
    "for name in name_list:\n",
    "    if name in current_model.vocab:\n",
    "        name_word_embedding[name] = current_model[name]\n",
    "len(name_word_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92292407",
   "metadata": {},
   "source": [
    "### Import common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7314dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_list = ['star', 'san', 'fish', 'policeman', 'luck', 'chair', 'woman', 'love', 'trust', 'cloud', 'cup', \n",
    "               'punishment', 'doctor', 'wealth', 'hand', 'sleep', 'success', 'money', 'horse', 'knowledge',\n",
    "               'rope', 'thief', 'laughter', 'snake', 'sun', 'map', 'meat', 'bread', 'respect', 'danger', 'poison',\n",
    "               'cat', 'bird', 'lake', 'heat', 'head', 'egg', 'tongue', 'smoke', 'story', 'dog', 'fruit', 'anger', \n",
    "               'music', 'death', 'heart', 'battle', 'freedom', 'crime', 'pain', 'sympathy', 'color', 'rain', 'ear',\n",
    "               'choice', 'husband', 'wind', 'wednesday', 'river', 'need', 'hunger', 'marriage', 'hair', 'author', \n",
    "               'fire', 'power', 'moon', 'pleasure', 'water', 'tree', 'life', 'peace', 'truth', 'girl', 'tooth',\n",
    "               'guilt', 'future', 'window', 'seed', 'picture', 'stone', 'courage', 'defeat', 'hope', 'book', 'knot',\n",
    "               'food', 'purpose', 'progress', 'root', 'work', 'friend', 'noise', 'game', 'belief', 'mother', \n",
    "               'father', 'house', 'fear', 'thunder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/stjepankusenic/POLAR_WEBE/data/raw/Common-eng-nouns2.txt') as f:\n",
    "    lines = f.readlines()\n",
    "lines=[line.rstrip('\\n') for line in lines]\n",
    "\n",
    "common_list=[]\n",
    "for word in lines:\n",
    "    if word in current_model.vocab:\n",
    "        common_list.append(word)\n",
    "\n",
    "#len(common_list2)\n",
    "#1521"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7008ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_word_embedding = dict()\n",
    "for name in common_list:\n",
    "    common_word_embedding[name] = current_model[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a30e10",
   "metadata": {},
   "source": [
    "### Create Polar Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a73ae1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "\n",
    "    temp_dict = dict()\n",
    "\n",
    "    embedding_size = current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "\n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "        temp_dict[each_word] = new_vector\n",
    "\n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35cc5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    new_embedding_dict = transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)\n",
    "\n",
    "    return new_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2ac3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_size = 500 # Number of POLAR dimenions\n",
    "antonym_vector_method = variance_antonymy_vector\n",
    "# random_antonym_vector or orthogonal_antonymy_vector or variance_antonymy_vector\n",
    "antonym_500 = [list_antonym[x] for x in antonym_vector_method[:500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5eed552c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model size is 249 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xy/yfm2dhtj3jvfy85hqy16wmk00000gn/T/ipykernel_3343/2385129348.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for each_word in tqdm(current_model):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8580ce390df466a8f13bc3d9624a14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create POLAR embedding of names\n",
    "name_new_embedding = generate_embedding_path(name_word_embedding, 'name_embeddings',True ,antonym_vector_method,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeac49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_polar_dict(company_name, antonym, embedding, top_n = False, n = 10):\n",
    "  temp_dict = dict()\n",
    "  temp_polar = embedding[company_name]\n",
    "\n",
    "  if top_n:\n",
    "    idx = np.argsort([abs(x) for x in temp_polar])[-n:]\n",
    "    for i in idx:\n",
    "      print(antonym[i],temp_polar[i],'\\n')\n",
    "\n",
    "\n",
    "  if len(antonym) == len(temp_polar):\n",
    "    for a in range(len(antonym)):\n",
    "      temp_dict[antonym[a]] = temp_polar[a]\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf67e72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('peon', 'shah') -0.10470107 \n",
      "\n",
      "('invalidate', 'support') -0.105401136 \n",
      "\n",
      "('abdicate', 'claim') 0.10544551 \n",
      "\n",
      "('biologic', 'geologic') 0.10603918 \n",
      "\n",
      "('elephant', 'flea') 0.106748015 \n",
      "\n",
      "('bothered', 'unaffected') -0.10811085 \n",
      "\n",
      "('petite', 'stout') -0.11053723 \n",
      "\n",
      "('representative', 'unassociated') -0.1143203 \n",
      "\n",
      "('hedge', 'squander') 0.120836124 \n",
      "\n",
      "('pontificate', 'withdraw') -0.13101497 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "facebook_polar = make_polar_dict('amazon', antonym_500, name_new_embedding, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693f12c",
   "metadata": {},
   "source": [
    "### Save the Polar Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b657075",
   "metadata": {},
   "outputs": [],
   "source": [
    "###make csv###\n",
    "# download csv\n",
    "\n",
    "df = dict()\n",
    "for t in name_list:\n",
    "    if t in current_model.vocab:\n",
    "        df[t] = make_polar_dict(t, antonym_500, name_new_embedding)\n",
    "\n",
    "new_df = pd.DataFrame(df).transpose()\n",
    "\n",
    "# change columns to better read names\n",
    "new_columns = []\n",
    "\n",
    "for pair in antonym_500:\n",
    "  temp = pair[0]+''+pair[1]\n",
    "  new_columns.append(temp)\n",
    "\n",
    "new_df.columns = new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab080fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('POLAR-Reddit-org-antonyms-inter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abde90",
   "metadata": {},
   "source": [
    "#### Next up generate insights with the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
