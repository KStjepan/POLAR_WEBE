{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a300c8",
   "metadata": {},
   "source": [
    "# Manual Evaluation of POLAR Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde60d5",
   "metadata": {},
   "source": [
    "## 1 Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38959af",
   "metadata": {},
   "source": [
    "### 1.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c3a8f",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import plotly\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from functools import partial\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import IntProgress\n",
    "from random import randint\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "import torch\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6b974e",
   "metadata": {},
   "source": [
    "### 1.2 Import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6915d4be",
   "metadata": {},
   "source": [
    "Import the embedding file you want to use for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bcf97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import embedding for analysis, need to do analysis for every combiantion of pre-trained model and antonyms used 8 in total\n",
    "company_df = pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/processed/POLAR-GloVeWiki-bus-antonyms-inter.csv')\n",
    "#Also input the embedding name and antonym set used here to get right file names in the end,\n",
    "#bus=business antonym set we created, org=original antonyms used by POLAR paper\n",
    "embedding_name = 'GloVeWiki_bus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3ceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose 10 companies that are in all embeddings for analysis\n",
    "#exchange for common-10-companies-google file when using googlenews embeddings\n",
    "with open(\"/Users/stjepankusenic/POLAR_WEBE/data/processed/common-10-companies\", \"rb\") as fp:  \n",
    "    b = pickle.load(fp)\n",
    "    \n",
    "analysis_df = company_df.loc[company_df['Unnamed: 0'].isin(b)]\n",
    "df_names = analysis_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256c9dcd",
   "metadata": {},
   "source": [
    "## 2 POLAR Embedding Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4549b14b",
   "metadata": {},
   "source": [
    "### 2.1 Create Test Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65310076",
   "metadata": {},
   "source": [
    "Functions for creating the test setup are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b681edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function that handles the POLAR pair search\n",
    "def get_important_pairs(df, number, name):\n",
    "    #get the number of highest absolute values\n",
    "    imp_row = df.loc[df['Unnamed: 0']==name]\n",
    "    imp_row = imp_row.loc[:,imp_row.columns!='Unnamed: 0']\n",
    "    imp_columns = imp_row.abs().values.argsort(1)[:, -number:][:, ::-1][0]\n",
    "    #get the column names\n",
    "    column_names=[]\n",
    "    column_list=imp_row.columns.values.tolist()\n",
    "    for i in imp_columns:\n",
    "        nam=column_list[i]\n",
    "        column_names.append(nam)\n",
    "    value_list=[]\n",
    "    for item in column_names:\n",
    "        value_list.append(imp_row[item].values)\n",
    "    ret_df=pd.DataFrame(column_names)\n",
    "    ret_df.columns=['top_polar_dim']\n",
    "    ret_df['top_value']=value_list\n",
    "    imp_columns_down=imp_row.abs().values.argsort(1)[:, :number][:, ::-1][0]\n",
    "    down_column_names=[]\n",
    "    down_value_list=[]\n",
    "    for i in imp_columns_down:\n",
    "        nam=column_list[i]\n",
    "        down_column_names.append(nam)\n",
    "        down_value_list.append(imp_row[nam].values)\n",
    "    ret_df['down_polar_dim']=down_column_names\n",
    "    ret_df['down_value']=down_value_list\n",
    "    \n",
    "    return ret_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset for the test\n",
    "def create_polar_intruder_dataset(number,bus_df,name):\n",
    "    company_data=get_important_pairs(bus_df,number,name)\n",
    "    rand_list=[]\n",
    "    pos_list=[]\n",
    "    name_list=[]\n",
    "    k = random.randint(0, number-1)\n",
    "    for i in range(number):\n",
    "        if i==k:\n",
    "            rand_list.append(company_data['down_polar_dim'].loc[i])\n",
    "            pos_list.append(k)\n",
    "            name_list.append(name)\n",
    "        else:\n",
    "            rand_list.append(company_data['top_polar_dim'].loc[i])\n",
    "            pos_list.append(k)\n",
    "            name_list.append(name)\n",
    "    company_data['random']= rand_list\n",
    "    company_data['name'] = name_list\n",
    "    company_data['position'] = pos_list\n",
    "    return company_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b28140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle intruder creation outside\n",
    "def create_polar_test_intruder_dataset(df):\n",
    "    intruder_list=[]\n",
    "    for bus in df:\n",
    "        lst=create_polar_intruder_dataset(5,company_df,bus)\n",
    "        intruder_list.append(lst)\n",
    "    intruder_list= pd.concat(intruder_list, ignore_index=True)\n",
    "    return intruder_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487d0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function for the test execution\n",
    "def polar_intrusion_test(df_intruder,name):\n",
    "\n",
    "    max_count = df_intruder.shape[0]\n",
    "    global i\n",
    "    i = 0\n",
    "    \n",
    "    button_0 = widgets.Button(description = df_intruder['random'].loc[i])\n",
    "    button_1 = widgets.Button(description = df_intruder['random'].loc[i+1])\n",
    "    button_2 = widgets.Button(description = df_intruder['random'].loc[i+2])\n",
    "    button_3 = widgets.Button(description = df_intruder['random'].loc[i+3])\n",
    "    button_4 = widgets.Button(description = df_intruder['random'].loc[i+4])\n",
    "    \n",
    "    global chosen_positions\n",
    "    chosen_positions=[]\n",
    "    \n",
    "    display(\"Polar Intrusion Text\")\n",
    "    \n",
    "    f = IntProgress(min=0, max=max_count)    \n",
    "    display(f)\n",
    "    \n",
    "    display(df_intruder['name'].loc[i])\n",
    "\n",
    "    display(button_0)\n",
    "    display(button_1)\n",
    "    display(button_2)\n",
    "    display(button_3)\n",
    "    display(button_4)\n",
    "    \n",
    "    def btn_eventhandler(position, obj):\n",
    "        global i \n",
    "        i += 5\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "            \n",
    "        display(\"Polar Intrusion Text\")\n",
    "        display(f)\n",
    "        f.value += 5\n",
    "        \n",
    "        global chosen_positions\n",
    "        chosen_positions+=  5*[position]\n",
    "                \n",
    "        if i < max_count:\n",
    "            \n",
    "            display(df_intruder['name'].loc[i])\n",
    "\n",
    "            button_0 = widgets.Button(description = df_intruder['random'].loc[i])\n",
    "            button_1 = widgets.Button(description = df_intruder['random'].loc[i+1])\n",
    "            button_2 = widgets.Button(description = df_intruder['random'].loc[i+2])\n",
    "            button_3 = widgets.Button(description = df_intruder['random'].loc[i+3])\n",
    "            button_4 = widgets.Button(description = df_intruder['random'].loc[i+4])\n",
    "            \n",
    "            display(button_0)\n",
    "            display(button_1)\n",
    "            display(button_2)\n",
    "            display(button_3)\n",
    "            display(button_4)\n",
    "            \n",
    "            button_0.on_click(partial(btn_eventhandler,0))\n",
    "            button_1.on_click(partial(btn_eventhandler,1))\n",
    "            button_2.on_click(partial(btn_eventhandler,2))\n",
    "            button_3.on_click(partial(btn_eventhandler,3))\n",
    "            button_4.on_click(partial(btn_eventhandler,4))\n",
    "            \n",
    "        else:\n",
    "            print (\"Thanks \" + name + \" you finished all the work!\")\n",
    "            #df_intruder['chosen_word'] = chosen_words\n",
    "            df_intruder['chosen_position'] = chosen_positions\n",
    "            df_intruder.to_csv(\"/Users/stjepankusenic/POLAR_WEBE/data/external/polar_intrusion_test_\" + name +'_'+ embedding_name + \"_results\" + \".csv\", index = False)\n",
    "                \n",
    "    button_0.on_click(partial(btn_eventhandler,0))\n",
    "    button_1.on_click(partial(btn_eventhandler,1))\n",
    "    button_2.on_click(partial(btn_eventhandler,2))\n",
    "    button_3.on_click(partial(btn_eventhandler,3))\n",
    "    button_4.on_click(partial(btn_eventhandler,4))\n",
    "\n",
    "    \n",
    "    return df_intruder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87163b",
   "metadata": {},
   "source": [
    "### 2.2 Test Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac12dbd3",
   "metadata": {},
   "source": [
    "Execute the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test =create_polar_test_intruder_dataset(df_names)\n",
    "#change the name to your first name!\n",
    "df_test1 =polar_intrusion_test(df_test,'Stjepan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3ed67",
   "metadata": {},
   "source": [
    "### 2.3 Evaluate the Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea18be8",
   "metadata": {},
   "source": [
    "Here we want to see how the annotators performed in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the test files you want to analyze\n",
    "data1 = pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/external/Coder-Evaluation-POLAR-dim/Sree-eval/polar_intrusion_test_Sreehari_GloVeWiki_org_results.csv')\n",
    "data2 = pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/external/Coder-Evaluation-POLAR-dim/Xho_eval/polar_intrusion_test_Xhoana_GloVeWiki_org_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that handles the evealuation\n",
    "def test_evaluation(df1,df2,number,name1, name2):\n",
    "    master_list1 = [name1]\n",
    "    master_list2 = [name2]\n",
    "    max_count= df1.shape[0]\n",
    "    list1=[]\n",
    "    list2=[]\n",
    "    for i in range(number):\n",
    "        list1.append(df1.loc[i*5])\n",
    "        list2.append(df2.loc[i*5])\n",
    "    df1= pd.DataFrame(list1)\n",
    "    df2= pd.DataFrame(list2)\n",
    "    df_both=pd.concat([df1,df2])\n",
    "    f21=f1_score(df1['chosen_position'], df1['position'], average='weighted')\n",
    "    print('F1 Score Coder 1:',f21)\n",
    "    master_list1.append(f21)\n",
    "    f22=f1_score(df2['chosen_position'], df2['position'], average='weighted')\n",
    "    print('F1 Score Coder 2:',f22)\n",
    "    master_list2.append(f22)\n",
    "    print( )\n",
    "    accuracy_score1=accuracy_score(df1['chosen_position'], df1['position'])\n",
    "    print('Accuracy Score Coder 1:',accuracy_score1)\n",
    "    master_list1.append(accuracy_score1)\n",
    "    accuracy_score2=accuracy_score(df2['chosen_position'], df2['position'])\n",
    "    print('Accuracy Score Coder 2:',accuracy_score2)\n",
    "    master_list2.append(accuracy_score2)\n",
    "    print( )\n",
    "    precision_score1=precision_score(df1['chosen_position'], df1['position'], average='weighted',zero_division=1)\n",
    "    print('Precision Score Coder 1:',precision_score1)\n",
    "    master_list1.append(precision_score1)\n",
    "    precision_score2=precision_score(df2['chosen_position'], df2['position'], average='weighted',zero_division=1)\n",
    "    print('Precision Score Coder 2:',precision_score2)\n",
    "    master_list2.append(precision_score2)\n",
    "    print( )\n",
    "    recall_score1=recall_score(df1['chosen_position'], df1['position'], average='weighted',zero_division=1)\n",
    "    print('Recall Score Coder 1:',recall_score1)\n",
    "    master_list1.append(recall_score1)\n",
    "    recall_score2=recall_score(df2['chosen_position'], df2['position'], average='weighted',zero_division=1)\n",
    "    print('Recall Score Coder 2:',recall_score2)\n",
    "    master_list2.append(recall_score2)\n",
    "    print( )\n",
    "    \n",
    "    kappa= cohen_kappa_score(df1['chosen_position'],df2['position'])\n",
    "    print('Cohens Kappa for the Coders:',kappa)\n",
    "    return master_list1, master_list2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a5bf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe and save initial results\n",
    "list1, list2 = test_evaluation(data1,data2,10,'Stjepan_GloVe_Twitter_bus','Stjepan_GloVe_Twitter_org')\n",
    "dataframe = pd.DataFrame([list1],columns=[\"Name\",\"F1_Score\",\"Accuracy_Score\",\"Precision_Score\",\"Recall_Score\"])\n",
    "dataframe.loc[len(dataframe)] = list2\n",
    "#dataframe.to_csv(\"/Users/stjepankusenic/POLAR_WEBE/data/processed/eval_results_individual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a48d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save further results into the same datadrame\n",
    "dataframe= pd.read_csv('/Users/stjepankusenic/POLAR_WEBE/data/processed/eval_results_individual')\n",
    "dataframe=dataframe.drop(columns=['Unnamed: 0'])\n",
    "list1, list2 = test_evaluation(data1,data2,10,'Simran_Reddit_bus','Simran_Reddit_org')\n",
    "dataframe.loc[len(dataframe)] = list1\n",
    "dataframe.loc[len(dataframe)] = list2\n",
    "display(dataframe)\n",
    "#dataframe.to_csv('/Users/stjepankusenic/POLAR_WEBE/data/processed/eval_results_individual')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
